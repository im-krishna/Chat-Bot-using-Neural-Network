{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Pre Processing data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kyada\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datapreprocessing import tokenize,stemmingText,bagOfWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>opening our json file</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intents.json','r') as f:\n",
    "    intents = json.load(f)\n",
    "    \n",
    "# print(intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a training corpus\n",
    "all_word = []\n",
    "training_data = []\n",
    "all_sentences = []\n",
    "tags = []\n",
    "for intent in intents['intents']:\n",
    "    for sen in intent['patterns']:\n",
    "        temp = tokenize(sen) #our tokenised sentence\n",
    "        temp2 = stemmingText(temp) #after stemming\n",
    "        all_word.extend(temp2)#final words are stemmed\n",
    "        tags.append(intent['tag'])\n",
    "        all_sentences.append(temp2)\n",
    "        training_data.append((temp2,intent['tag']))\n",
    "        \n",
    "all_word = sorted(set(all_word))\n",
    "\n",
    "\n",
    "# all_word is all set of unique words in our training corpus\n",
    "# this will be used to create embedding using bag of words\n",
    "\n",
    "# we also have our training data which has sentences and their corresponding tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['delivery', 'funny', 'goodbye', 'greeting', 'items', 'payments', 'thanks']\n"
     ]
    }
   ],
   "source": [
    "tags = sorted(list(set(tags)))\n",
    "print(tags)\n",
    "\n",
    "#here we use our label encoding \n",
    "#0-goodbye\n",
    "#1-thanks\n",
    "#2-funny|\n",
    "# etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>For bag of words embedding</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 1]]\n",
      "[3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 6 6 6 6 6 6 6 4 4 4 4 4 4 4 5 5 5 5 5 5 5\n",
      " 5 5 0 0 0 0 0 0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#creating our embedded data\n",
    "import numpy as np\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for (sen,tag) in training_data:\n",
    "    temp = np.array(bagOfWords(sen,all_word))\n",
    "    X_train.append(temp) #collection of numpy array\n",
    "    y_train.append(tags.index(tag))\n",
    "    \n",
    "    \n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "#uptil here we are clear we have our bag of words basically embedded sentence and it's output tag whic would be passed in the neural network all stores as numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>For Tf-IDF embedding</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.30546219 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.24436975 0.         0.         ... 0.         0.         0.11056839]]\n",
      "[3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 6 6 6 6 6 6 6 4 4 4 4 4 4 4 5 5 5 5 5 5 5\n",
      " 5 5 0 0 0 0 0 0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#creating our embedded data\n",
    "import numpy as np\n",
    "from tfIdf import tf_idf\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for (sen,tag) in training_data:\n",
    "    y_train.append(tags.index(tag))\n",
    "    \n",
    "X_train = tf_idf(all_sentences,all_word)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "# print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Converting everything to pytorch dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyada\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inherting Dataset class\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    # returns (features,tag) at index\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "dataset = ChatDataset()\n",
    "#train_loader is our iterator of all dataset divided into groups of 8 batch_size\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "\n",
    "#train_loader class has automatically added a dimension to our numpy array and converted it into tensor \n",
    "#train loader has batches of 8 as dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Creating Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 7\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters \n",
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 8\n",
    "output_size = len(tags)\n",
    "print(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size,hidden_size,output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we define our loss function and cross entropy\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 1.5414\n",
      "Epoch [200/1000], Loss: 0.1567\n",
      "Epoch [300/1000], Loss: 0.0371\n",
      "Epoch [400/1000], Loss: 0.0026\n",
      "Epoch [500/1000], Loss: 0.0032\n",
      "Epoch [600/1000], Loss: 0.0017\n",
      "Epoch [700/1000], Loss: 0.0006\n",
      "Epoch [800/1000], Loss: 0.0001\n",
      "Epoch [900/1000], Loss: 0.0001\n",
      "Epoch [1000/1000], Loss: 0.0001\n",
      "final loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs): #for all epochos\n",
    "    for (words, labels) in train_loader: #for all batches\n",
    "        # print((words,labels))\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(dtype=torch.long).to(device)\n",
    "        # print(words,labels)\n",
    "        # Forward pass\n",
    "        outputs = model.ffnn(words) #this is our output from the feed forward neural network\n",
    "        # model\n",
    "        # print(outputs)\n",
    "        # print(labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # we calculate the loss \n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "print(f'final loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Saving the Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"model_state\" : model.state_dict(),\n",
    "    \"input_size\" : input_size,\n",
    "    \"output_size\" : output_size,\n",
    "    \"hidden_size\" : hidden_size,\n",
    "    \"all_words\" : all_word,\n",
    "    \"tags\" : tags\n",
    "}\n",
    "\n",
    "#this is all the data we want to store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete.file save to dataTFIDF.pth\n"
     ]
    }
   ],
   "source": [
    "File = \"dataTFIDF.pth\"\n",
    "torch.save(data, File)\n",
    "print(f'training complete.file save to {File}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6711313071efc63eb5adf6b9ea2d1a74e10dd6e595c82696b88a35d0bda84b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
